from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neural_network import MLPRegressor
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

def get_features(df, mode="fisik"):
    if mode == "fisik":
        features = ['Altitude', 'Coffee Age', 'Moisture %',
                    'Category One Defects', 'Category Two Defects', 'Quakers']
    else:
        features = ['Uniformity', 'Clean Cup', 'Sweetness',
                    'Overall','Flavor','Aftertaste','Balance','Acidity','Aroma','Body']
        features = [f for f in features if f in df.columns]
    return features

def prepare_data(df, features):
    X = df[features + [c for c in ['Processing Method','Variety'] if c in df.columns]]
    y = df['Total Cup Points'] if 'Total Cup Points' in df.columns else np.zeros(len(df))
    
    numeric_features = features
    categorical_features = [c for c in ['Processing Method','Variety'] if c in df.columns]

    preprocessor = ColumnTransformer([
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),
        ('num', StandardScaler(), numeric_features)
    ])

    X_processed = preprocessor.fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)
    
    return X_train, X_test, y_train, y_test, preprocessor

def benchmark_models(X_train, X_test, y_train, y_test):
    models = {
        "Linear Regression": LinearRegression(),
        "Ridge": Ridge(),
        "Lasso": Lasso(),
        "ElasticNet": ElasticNet(),
        "Bayesian Ridge": BayesianRidge(),
        "Random Forest": RandomForestRegressor(n_estimators=200, random_state=42),
        "Gradient Boosting": GradientBoostingRegressor(n_estimators=200, random_state=42),
        "XGBoost": xgb.XGBRegressor(n_estimators=200, random_state=42, verbosity=0),
        "LightGBM": lgb.LGBMRegressor(n_estimators=200, random_state=42),
        "CatBoost": cb.CatBoostRegressor(n_estimators=200, verbose=0, random_state=42),
        "SVR": SVR(),
        "KNN": KNeighborsRegressor(),
        "MLP": MLPRegressor(hidden_layer_sizes=(50,50), max_iter=500, random_state=42)
    }

    results = {}
    for name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        results[name] = {
            "MAE": mean_absolute_error(y_test, y_pred),
            "RMSE": np.sqrt(mean_squared_error(y_test, y_pred)),
            "R2": r2_score(y_test, y_pred)
        }
    results_df = pd.DataFrame(results).T.sort_values(by='RMSE')
    best_model_name = results_df.index[0]
    best_model = models[best_model_name]

    return best_model, results_df
